{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and clean Trillium pusillum complex occurrence records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and Organize the DWC-A formatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shaped as: (425, 88)\n"
     ]
    }
   ],
   "source": [
    "# the initial symbiota dataset from SERNEC\n",
    "df = pd.read_csv('occurrences/SymbOutput_2020-10-13_101423_DwC-A/occurrences.csv',\n",
    "                  encoding=\"iso-8859-1\",low_memory=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# an addendum including BRIT's texanum records\n",
    "df2 = pd.read_csv('occurrences/BRIT_texanum_SymbOutput_2020-11-02_152303_DwC-A/occurrences.csv',\n",
    "                  encoding=\"iso-8859-1\",low_memory=False, quoting=csv.QUOTE_ALL)\n",
    "# combine both\n",
    "df = df.append(df2,ignore_index=True)\n",
    "# drop any duplicates (based on the 'id' col)\n",
    "df.drop_duplicates(subset='id', keep='last',inplace=True)\n",
    "\n",
    "print(f\"dataframe shaped as: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean out cultivated occurrences\n",
    "Very few records are caught up in this cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shaped as: (422, 88)\n"
     ]
    }
   ],
   "source": [
    "# align \"0\" to null (i.e., not cultivated)\n",
    "df.loc[df['cultivationStatus']==0, 'cultivationStatus'] = np.nan\n",
    "# remove anything with data in cultivated\n",
    "df = df[df['cultivationStatus'].isna()]\n",
    "\n",
    "# remove botanical garden occurrences (based on locality)\n",
    "df = df[~df[\"locality\"].str.lower().str.contains(\"botanical|garden|cultivate\", na=False)]\n",
    "# remove Pennsylvania occurrences\n",
    "df = df[~df['stateProvince'].str.lower().str.contains('pennsylvania', na=False)]\n",
    "\n",
    "print(f\"dataframe shaped as: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include iNat observations\n",
    "Combine SERNEC occurences with iNat observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shaped as: (545, 122)\n"
     ]
    }
   ],
   "source": [
    "inat = pd.read_csv(\"occurrences/iNat_pusillum_observations_109874.csv\",encoding='utf-8',\n",
    "                   low_memory=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# rename a few iNat columns\n",
    "# lat / long are being renamed so as NOT to conflict (and therefore overwrite)\n",
    "# existing geospatial data\n",
    "iNat_alignment = {\"latitude\":\"LATITUDE\",\n",
    "                 \"longitude\":\"LONGITUDE\",\n",
    "                 \"positional_accuracy\":\"coordinateUncertaintyInMeters\",\n",
    "                 \"scientific_name\":\"scientificName\"}\n",
    "inat = inat.rename(iNat_alignment, axis=1)\n",
    "\n",
    "# add and populate a new column named \"source\" to both the SERNEC & iNat dataframes\n",
    "inat['source'] = \"iNat\"\n",
    "df['source'] = 'SERNEC'\n",
    "# merge the two dataframes\n",
    "df = pd.concat([df, inat], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"dataframe shaped as: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up sci name strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sci_name cleaning: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Trillium pusillum var. virginianum',\n",
       " 'Trillium pusillum',\n",
       " 'Trillium pusillum var. ozarkanum',\n",
       " 'Trillium pusillum var. monticulum',\n",
       " 'Trillium pusillum var. pusillum',\n",
       " 'Trillium pusillum var. alabamicum',\n",
       " 'Trillium pusillum var. texanum',\n",
       " 'Trillium ozarkanum',\n",
       " 'TRILLIUM PUSILLUM MICHX.',\n",
       " 'TRILLIUM PUSILLUM MICHX. var. OZARKANUM (PALMER & STEYERMARK) STEYERMARK',\n",
       " 'TRILLIUM PUSILLUM MICHX. var. VIRGINIANUM FERN.',\n",
       " 'TRILLIUM PUSILLUM MICHX. var. PUSILLUM',\n",
       " 'Trillium pusillum var. monticolum',\n",
       " 'Trillium texanum',\n",
       " 'TRILLIUM TEXANUM BUCKL.',\n",
       " 'Trillium pusillum ozarkanum',\n",
       " 'Trillium pusillum pusillum',\n",
       " 'Trillium pusillum texanum',\n",
       " 'Trillium pusillum virginianum']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "after sci_name cleaning: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Trillium pusillum virginianum',\n",
       " 'Trillium pusillum',\n",
       " 'Trillium pusillum ozarkanum',\n",
       " 'Trillium pusillum monticulum',\n",
       " 'Trillium pusillum pusillum',\n",
       " 'Trillium pusillum alabamicum',\n",
       " 'Trillium pusillum texanum',\n",
       " 'Trillium ozarkanum',\n",
       " 'Trillium pusillum monticolum',\n",
       " 'Trillium texanum']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first examine the current set of sciNames\n",
    "print(\"before sci_name cleaning: \")\n",
    "display(df['scientificName'].unique().tolist())\n",
    "print()\n",
    "def sci_name_cleaner(rowdata):\n",
    "    sci_name = rowdata['scientificName'].lower()\n",
    "    # establish a list of substrings to remove\n",
    "    # this list was derived after examining the results of previous iterations\n",
    "    filter_words = ['var.', 'michx.', 'buckl.', 'fern.', '(palmer & steyermark) steyermark']\n",
    "    # replace each substring with empty string (i.e., \"\")\n",
    "    for word in filter_words:\n",
    "        if word in sci_name:\n",
    "            sci_name = sci_name.replace(word, '')\n",
    "    \n",
    "    # basic string cleaning\n",
    "    sci_name = sci_name.replace('   ', ' ')\n",
    "    sci_name = sci_name.replace('  ', ' ')\n",
    "    sci_name = sci_name.strip()\n",
    "    sci_name = sci_name.capitalize()\n",
    "    rowdata['scientificName'] = sci_name\n",
    "    return rowdata\n",
    "\n",
    "df = df.apply(sci_name_cleaner, axis=1)            \n",
    "\n",
    "# examine set of sciNames after cleaning\n",
    "print(\"after sci_name cleaning: \")\n",
    "display(df['scientificName'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean county names & align coords to administrative centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 of 545 rows are missing geopoints\n"
     ]
    }
   ],
   "source": [
    "# identify how many occurrences are missing lat/lon data\n",
    "missing_qty = df.shape[0] - df[~df['decimalLatitude'].isna()].shape[0]\n",
    "print(f\"{missing_qty} of {df.shape[0]} rows are missing geopoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the administrative centroid data\n",
    "\n",
    "# centroid coordinates reference file\n",
    "# source: https://data.healthcare.gov/dataset/Geocodes-USA-with-Counties/52wv-g36k/data\n",
    "counties_ref = pd.read_csv(\"ref/Geocodes_USA_with_Counties.csv\") #centroid coordinates\n",
    "counties_ref['county'] = counties_ref['county'].str.lower() # convert to lower case for ease of alignment\n",
    "\n",
    "# simple state name to abbreviation reference csv.\n",
    "state_codes = pd.read_csv(\"ref/state_codes.csv\") #state codes used for looking up centroids\n",
    "state_codes['State'] = state_codes['State'].str.lower() # convert to lower case for ease of alignment\n",
    "\n",
    "\n",
    "def clean_county_str(rowdata):\n",
    "    # a function used to clean county/state names and attempt to look up a centroid based on them\n",
    "    try:\n",
    "        county_str = rowdata['county'].lower().strip()\n",
    "        cutlist = ['co.', 'county']\n",
    "        for x in cutlist:\n",
    "            county_str.replace(x, '')\n",
    "        county_str = county_str.rstrip()\n",
    "\n",
    "        state_name = rowdata['stateProvince'].lower().strip()\n",
    "        st_code = state_codes.loc[state_codes['State'] == state_name, 'Code'].tolist()[0]\n",
    "        \n",
    "        # limit the reference dataframe to only administrative entities within the\n",
    "        # state/county combination.\n",
    "        coords = counties_ref.loc[(counties_ref['county'] == county_str)\n",
    "                                  & (counties_ref['state'] == st_code)]\n",
    "\n",
    "        # since multiple administration levels may exist, accept the centroid as the\n",
    "        # median lat/lon of all results\n",
    "        lat = coords['latitude'].median()\n",
    "        lon = coords['longitude'].median()\n",
    "\n",
    "        # store the centroid coordinates to the row being processed\n",
    "        rowdata['decimalLatitude'] = lat\n",
    "        rowdata['decimalLongitude'] = lon\n",
    "    except AttributeError as e:\n",
    "        # AttributeErrors likely due to a NaN value where a string was expected\n",
    "        # in this case, no changes are made to the decimalLatitude, decimalLongitude fields.\n",
    "        pass\n",
    "    return rowdata\n",
    "\n",
    "# apply the clean_county_str function to the entire df row-by-row\n",
    "df = df.apply(clean_county_str, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplify & save final output\n",
    "\n",
    "'LATITUDE' and 'LONGITUDE' were imported from the iNat records. Fill any empty lat/long data with the decimalLatitude, decimalLongitude cols. Then, simplify the output for delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 22)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LATITUDE'].fillna(df['decimalLatitude'], inplace=True)\n",
    "df['LONGITUDE'].fillna(df['decimalLongitude'], inplace=True)\n",
    "\n",
    "# remove any without coords\n",
    "df = df[df['LATITUDE'].notna()]\n",
    "\n",
    "# limit the output to columns most likely to be useful downstream\n",
    "keep_cols = ['LATITUDE',\n",
    "            'LONGITUDE',\n",
    "            'occurrenceID',\n",
    "            'recordId',\n",
    "            'collectionCode',\n",
    "            'catalogNumber',\n",
    "            'source',\n",
    "            'family',\n",
    "            'genus',\n",
    "            'taxonID',\n",
    "            'taxonRank',\n",
    "            'taxonRemarks',\n",
    "            'scientificName',\n",
    "            'scientificNameAuthorship',\n",
    "            'identificationQualifier',\n",
    "            'identificationReferences',\n",
    "            'identificationRemarks',\n",
    "            'identifiedBy',\n",
    "            'dateIdentified',\n",
    "            'coordinateUncertaintyInMeters',\n",
    "            'locality',\n",
    "            'county']\n",
    "\n",
    "# simplify cols\n",
    "df = df[keep_cols].copy()\n",
    "# save output\n",
    "df.to_csv(\"pusillum_points.csv\")\n",
    "# check shape\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
